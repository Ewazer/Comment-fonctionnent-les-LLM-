# Le fonctionnement des LLM de type GPT

# Introduction

Dans ce guide je vais essayer de vous faire comprendre de fa√ßon d√©taill√©e comment fonctionne le syst√®me GPT (**generative pre-trained transformer**) il en existe de plein de type qui ont des fonctions diff√©rentes (BioGPT (biom√©decine) [ProGPT2](https://huggingface.co/nferruz/ProtGPT2), [ChatGPT](https://chatgpt.com/) (mod√®le g√©n√©ral) mais nous allons nous focaliser plut√¥t sur une architecture proche de GPT 2.

Le seul pr√©requis est d‚Äôavoir d√©j√† utilis√© [ChatGPT](https://chatgpt.com/). L‚Äôid√©e de ce guide est d‚Äô√™tre simple et d√©taill√© √† la fois en vulgarisant le moins possible.

# Les Datasets

## Qu‚Äôest-ce qu‚Äôun dataset et √† quoi √ßa sert ?

Un dataset est un **ensemble de donn√©es structur√©es**, comme par exemple :

- un ensemble de photos de pingouins üêß,

- un ensemble de tous les √©crits de Shakespeare üìñ,

- ou encore une liste de tous les repas d‚Äôun individu pendant un an ü•ò.

Ces donn√©es peuvent √™tre **labellis√©es**, c‚Äôest-√†-dire que chaque donn√©e est associ√©e √† une √©tiquette. Si on reprend l‚Äôexemple du dataset contenant tous les repas d‚Äôun individu pendant un an, chaque repas peut √™tre labellis√© en associant le jour et le moment de la journ√©e o√π le repas a √©t√© pris.

Les datasets sont utilis√©s dans divers domaines comme le **machine learning** ou encore la **cr√©ation de bases de donn√©es**.

Quand un dataset contient un ensemble de textes on parle de **corpus**.
## Quel r√¥le jouent les datasets dans l‚Äôentra√Ænement des mod√®les GPT ?


Les datasets sont le composant le plus important des mod√®les de langage car ils forgent **le style et la qualit√© du langage** et **les connaissances d‚Äôun mod√®le**.
## WebText

Les mod√®les au-del√† de GPT 2 de chez open ai n‚Äôont pas √©t√© entra√Æn√©s sur des datasets comme Wikip√©dia mais sur **WebText**.

WebText contient le contenu de tous les **liens sortants de Reddit avec au moins 3 karmas (likes)**.

WebText est bas√© sur **45 millions de liens**, dont **8 millions de documents** pour un total de **40 Go de texte**.

OpenAI a fait √ßa pour mettre l‚Äô**accent sur la qualit√© des donn√©es**.

Il est important de noter que tous les documents provenant de Wikip√©dia ne font pas partie de WebText.

Cela est d√ª √† deux raisons principales :

- Pour diversifier la langue (Wikip√©dia a un langage neutre et tr√®s structur√©),

- Pour √©viter les r√©p√©titions de donn√©es.

WebText **n‚Äôest pas open source** (public), mais il existe des datasets similaires open source comme [OpenWebText](https://github.com/jcpeterson/openwebtext) ou encore [The Pile](https://pile.eleuther.ai/).

## Liste de datasets que vous pouvez utiliser 

On choisit souvent un dataset en fonction de **sa taille** et du **type de langage** souhait√©.

Datasets l√©gers: 

- **Tiny Shakespeare** : [https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) (l√©ger, il est conseill√© pour faire des petits mod√®les)

- **Wikipedia** : [https://huggingface.co/datasets/openskyml/wikipedia](https://huggingface.co/datasets/openskyml/wikipedia) (disponible en plusieurs langages)

- **Open Artificial Knowledge** : https://oakdataset.org/ (les donn√©es ont √©t√© r√©cup√©r√©es gr√¢ce √† des IA comme ChatGPT, Claude et Gemini)

Datasets plus lourds: 

- **OpenWebText** : https://github.com/jcpeterson/openwebtext (version open source de WebText de 38 Go)

- **The Pile** : https://pile.eleuther.ai (corpus de 886 go utilis√© pour des mod√®les comme GPT-Neo et LLaMa)

# Tokenisation 

## La Tokenisation c‚Äôest quoi ?

Pour comprendre un texte, notre mod√®le GPT a besoin d‚Äô**effectuer des op√©rations math√©matiques sur les mots**. Mais sur du texte brut c‚Äôest juste impossible. Pour cela on a besoin de **tokeniser** notre texte. C‚Äôest √† dire qu‚Äôon va **transformer et d√©couper en unit√©s le texte pour l‚Äôadapter** √† notre mod√®le. On dit que le texte est d√©coup√© en **token**.

Dans le cas d‚Äôun mod√®le comme GPT, celui-ci va dans un premier temps **prendre un corpus de texte et le diviser en unit√©s**. 
Chaque unit√© est ajout√©e √† une **liste de vocabulaire** **avec un identifiant** s‚Äôil n‚Äôexiste pas. 

Puis notre mod√®le est **pr√™t** √† tokeniser. Le texte sera d√©coup√© en unit√©s chaque unit√© dans le texte est remplac√©e par le num√©ro associ√©. 

![Miro Image](https://miro.medium.com/v2/resize:fit:828/format:webp/1*gWP5Whykah1101EpYy17qQ.png)

*Source Image : https://teetracker.medium.com/llm-fine-tuning-step-tokenizing-caebb280cfc2

La question principale est donc comment allons-nous d√©couper notre texte. Nous allons explorer les diff√©rentes approches qui s‚Äôoffrent √† nous.
## Les tokens sp√©ciaux dans la tokenisation

Dans le vocabulaire en plus des unit√©s il y a des tokens sp√©ciaux en voici une liste:

- `[CLS]` / `[SEP]` Marquent le **d√©but/fin** de la s√©quence
- `[UNK]` Remplace **les unit√©s inconnues** dans le vocabulaire
- `[PAD]` Lors de l‚Äôentra√Ænement d‚Äôun mod√®le si l'on veut pouvoir g√©rer **plusieurs phrases** en m√™me temps il faut qu‚Äôelles aient la **m√™me longueur**. Pour cela, on ajoute le token `[PAD]` √† la fin des phrases les plus courtes. Il faudra quand m√™me sp√©cifier au mod√®le de ne pas faire attention √† `[PAD]` et l‚Äôignorer dans son entra√Ænement. 

Exemple `[PAD]`:
```
s√©quence 1 : [[CLS], "Bonjour", "tout", "le", "monde", [SEP]]  
s√©quence 2 : [[CLS], "Salut", [SEP], [PAD], [PAD], [PAD]]
```

### Bonus technique:

Pourquoi faut il que les phrases aient la m√™me longueur quand elles sont par lot ?

1. Les tenseurs **ne peuvent pas avoir des lignes de longueurs diff√©rentes**

Les tenseurs sont des **types d‚Äôobjets optimis√©s pour les calculs en python** et ils ne peuvent pas contenir des lignes avec des tailles diff√©rentes.

2. On doit **conna√Ætre √† l‚Äôavance les dimensions d‚Äôune matrice** si on veut faire un produit matriciel ou un softmax

Nous verrons plus tard que pour notre mod√®le Transformer le mod√®le doit conna√Ætre √† l‚Äôavance la **taille du batch** (la taille du lot que notre mod√®le va ‚Äôabsorber‚Äô √† chaque √©tape de notre entra√Ænement), justement la **taille des s√©quences dans les lots** et enfin les **dimensions vectorielles de chaque token** mais nous verrons cela plus en d√©tail plus tard.

### La Tokenisation par mot

Une m√©thode est de **diviser notre texte par mots**.

On commence par d√©finir notre vocabulaire pour cela, imaginons que nous prenons juste une phrase

```python
"Le chat mange la souris"
```

Le vocabulaire sera donc : 

```python
{0: 'souris', 1: 'chat', 2: 'le', 3: 'la', 4: '.', 5: 'mange', 6: '[UNK]', 7: '[PAD]', 8: '[CLS]', 9: '[SEP]'}
```

Imaginons que nous essayons de tokeniser avec notre mod√®le la phrase <mark style="background: #BBFABBA6;">"La souris mange le chat."</mark>, le r√©sultat sera donc <mark style="background: #BBFABBA6;">[8, 2, 1, 6, 6, 2, 6, 4, 9]</mark>

```python
‚û°Ô∏è "La souris mange le chat."
‚û°Ô∏è ['[CLS]', 'la', 'souris', 'mange', 'le', 'chat', '.', '[SEP]']
‚û°Ô∏è [8, 3, 0, 5, 2, 1, 4, 9]
```

### Les probl√®mes de cette m√©thode

Prenons une autre phrase :

<mark style="background: #FFB86CA6;">"Le chat est sur le canap√©."</mark>

Le r√©sultat de la tokenisation sera :  
<mark style="background: #FF5582A6;">['[CLS]', 'le', 'chat', '[UNK]', '[UNK]', 'le', '[UNK]', '.', '[SEP]']</mark>  

ce qui donne :  
<mark style="background: #CACFD9A6">[8, 2, 1, 6, 6, 2, 6, 4, 9]</mark>

Si un mot n‚Äôest **pas exactement dans le vocabulaire**, il sera totalement **perdu et remplac√©** par `[UNK]`ce qui constitue une **perte d‚Äôinformation** pour notre mod√®le. Le **vocabulaire a donc besoin de contenir √©norm√©ment de mots**, ce qui le rendrait √©norme, ce qui est un inconv√©nient pour les calculs car il occuperait **beaucoup de m√©moire**. De plus le mod√®le **ne comprendrait pas les mots avec des fautes d‚Äôorthographe**. 

### Bonus : Impl√©mentation en python

```python
import re

#Prenons une seule phrase comme corpus
corpus = "Le chat mange la souris."
  
#On va s√©parer les ponctuations
corpus = re.sub(r"([?!.,;:])", r" \1 ", corpus)

#Imaginons qu‚Äôune unit√© corresponde √† un mot
#Commen√ßons par d√©finir un vocabulaire associant chaque mot √† un num√©ro
vocabulaire = {i: word for i, word in enumerate(set(corpus.lower().split()))}
#‚ö†Ô∏è J‚Äôutilise set donc l‚Äôordre des tokens normaux n‚Äôest pas d√©terministe (car un set en Python est impl√©ment√© comme une table de hachage)

#Puis ajoutons les tokens sp√©ciaux
vocabulaire[len(vocabulaire)] = '[UNK]' #pour les mots inconnus
vocabulaire[len(vocabulaire)] = '[PAD]' #pour le padding
vocabulaire[len(vocabulaire)] = '[CLS]' #pour le d√©but de phrase
vocabulaire[len(vocabulaire)] = '[SEP]' #pour la fin de phrase

#Notre vocabulaire est donc {0: 'souris', 1: 'chat', 2: 'le', 3: 'la', 4: '.', 5: 'mange', 6: '[UNK]', 7: '[PAD]', 8: '[CLS]', 9: '[SEP]'}

vocabulaire_inverse = {word: i for i, word in vocabulaire.items()}
#{'souris': 0, 'chat': 1, 'le': 2, 'la': 3, '.': 4, 'mange': 5, '[UNK]': 6, '[PAD]': 7, '[CLS]': 8, '[SEP]': 9}

#puis d√©finissons une fonction qui va d√©couper et transformer notre texte
def tokenize(text):
¬† ¬† #On va d‚Äôabord le mettre en minuscule
¬† ¬† text = text.lower()

¬† ¬† #On va ensuite s√©parer les ponctuations
¬† ¬† text = re.sub(r"([?!.,;:])", r" \1 ", text)

¬† ¬† #On va ensuite s√©parer les mots
¬† ¬† text = text.split()

¬† ¬† #on va ajouter les tokens sp√©ciaux
¬† ¬† text = ['[CLS]'] + ['[UNK]' if w not in vocabulaire_inverse else w for w in text] + ['[SEP]']

¬† ¬† #On va ensuite transformer les mots en id
¬† ¬† text_tokenise = [vocabulaire_inverse[word] for word in text]

¬† ¬† return text_tokenise
  
print(tokenize("La souris mange le chat."))
#['[CLS]', 'la', 'souris', 'mange', 'le', 'chat', '.', '[SEP]'] ¬†=> [8, 3, 0, 5, 2, 1, 4, 9]

print(tokenize("Le chat est sur le canap√©."))
#['[CLS]', 'le', 'chat', '[UNK]', '[UNK]', 'le', '[UNK]', '.', '[SEP]'] => [8, 2, 1, 6, 6, 2, 6, 4, 9]
```

## La Tokenisation par caract√®re

Une autre approche est la tokenisation par caract√®re. C‚Äôest √† dire de **diviser le texte par caract√®re √† la place de mots**. Cette approche **r√©duirait consid√©rablement la taille du vocabulaire et garantit que la plupart du vocabulaire soit reconnu**.

### Les Inconv√©nients de cette approche 

Il y a deux gros inconv√©nients:

- Les s√©quences deviennent tr√®s longues car elles sont d√©coup√©s plus finement.  
‚û°Ô∏è <mark style="background: #FFB86CA6;">"Le chat est sur le canap√©."</mark>
‚û°Ô∏è <mark style="background: #CACFD9A6;">[36, 17, 10, 0, 8, 13, 6, 25, 0, 10, 24, 25, 0, 24, 26, 23, 0, 17, 10, 0, 8, 6, 19, 6, 21, 32, 0, 4, 0, 37]</mark>

- C‚Äôest plus compliqu√© pour un mod√®le de comprendre la relation entre les caract√®res que la relation entre les mots.
### Bonus : Impl√©mentation en python

```python
import re

#Prenons comme corpus seulement une phrase
corpus = "Hier, au zoo, j‚Äôai vu dix gu√©pards, cinq z√©bus, un yak et le wapiti fumer. ?!‚Ä¶"
#Cette phrase a la particularit√© de contenir toutes les lettres de l'alphabet et quelques ponctuations

#On va s√©parer les ponctuations
corpus = re.sub(r"([?!.,;:])", r" \1 ", corpus)

#On commence par d√©finir un vocabulaire en associant un identifiant √† chaque caract√®re
vocabulaire = {i : word for i, word in enumerate(sorted(set(corpus.lower())))}
#{0: ' ', 1: '!', 2: "'", 3: ',', 4: '.', 5: '?', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z', 32: '√©', 33: '‚Ä¶'}

#Puis ajoutons les tokens sp√©ciaux
vocabulaire[len(vocabulaire)] = '[UNK]' #pour les mots inconnus
vocabulaire[len(vocabulaire)] = '[PAD]' #pour le padding
vocabulaire[len(vocabulaire)] = '[CLS]' #pour le d√©but de phrase
vocabulaire[len(vocabulaire)] = '[SEP]' #pour la fin de phrase

#Nous nous retrouvons donc avec un vocabulaire de 37 √©l√©ments
#{0: ' ', 1: '!', 2: "'", 3: ',', 4: '.', 5: '?', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z', 32: '√©', 33: '‚Ä¶', 34: '[UNK]', 35: '[PAD]', 36: '[CLS]', 37: '[SEP]'}

vocabulaire_inverse = {word: i for i, word in vocabulaire.items()}

#puis d√©finissons une fonction qui va d√©couper et transformer notre texte
def tokenize(text):
¬† ¬† #On va d‚Äôabord le mettre en minuscule
¬† ¬† text = text.lower()

¬† ¬† #On va ensuite s√©parer les ponctuations
¬† ¬† text = re.sub(r"([?!.,;:])", r" \1 ", text)

¬† ¬† #on va ajouter les tokens sp√©ciaux
¬† ¬† text = ['[CLS]'] + ['[UNK]' if w not in vocabulaire_inverse else w for w in text] + ['[SEP]']

¬† ¬† #On va ensuite transformer les mots en id
¬† ¬† text_tokenise = [vocabulaire_inverse[word] for word in text]

¬† ¬† return text_tokenise

print(tokenize("Le chat mange la souris."))
# => ['[CLS]', 'l', 'e', ' ', 'c', 'h', 'a', 't', ' ', 'm', 'a', 'n', 'g', 'e', ' ', 'l', 'a', ' ', 's', 'o', 'u', 'r', 'i', 's', ' ', '.', ' ', '[SEP]']
# => [36, 17, 10, 0, 8, 13, 6, 25, 0, 18, 6, 19, 12, 10, 0, 17, 6, 0, 24, 20, 26, 23, 14, 24, 0, 4, 0, 37]

print(tokenize("Le chat est sur le canap√©."))
# => ['[CLS]', 'l', 'e', ' ', 'c', 'h', 'a', 't', ' ', 'e', 's', 't', ' ', 's', 'u', 'r', ' ', 'l', 'e', ' ', 'c', 'a', 'n', 'a', 'p', '√©', ' ', '.', ' ', '[SEP]']
# => [36, 17, 10, 0, 8, 13, 6, 25, 0, 10, 24, 25, 0, 24, 26, 23, 0, 17, 10, 0, 8, 6, 19, 6, 21, 32, 0, 4, 0, 37]
```

## Tokenisation BPE (Byte Pair Encoding) 

Le BPE est une m√©thode de tokenisation par **subwords** (c‚Äôest-√†-dire **des sous-mots**). Au d√©but, **BPE d√©coupe en caract√®re et petit √† petit ajoute les paires les plus fr√©quentes**. Ainsi, BPE a pour objectif de r√©duire le nombre de tokens n√©cessaire pour tokeniser une phrase sans perdre en pr√©cision.

On commence avec un vocabulaire avec tous les caract√®res de base du corpus et on traite le corpus pour qu‚Äôil soit repr√©sent√© comme une liste de tokens

```python
Corpus = "hug", "pug", "pun", "bun", "hugs"

New_Corpus = ['h', 'u‚Äô, ‚Äôg‚Äô, ‚Äô</w>‚Äô,‚Äôp‚Äô, ‚Äôu‚Äô, ‚Äôg‚Äô, ‚Äô</w>‚Äô,‚Äôp‚Äô, ‚Äôu‚Äô, ‚Äôn‚Äô, ‚Äô</w>‚Äô,‚Äôb‚Äô, ‚Äôu‚Äô, ‚Äôn‚Äô, ‚Äô</w>‚Äô,‚Äôh‚Äô, ‚Äôu‚Äô, ‚Äôg‚Äô, ‚Äôs‚Äô, ‚Äô</w>‚Äô]

notre_vocabulaire_de_base = ["b", "g", "h", "n", "p", "s", "u"]
```

Puis on se met √† compter les paires et leurs fr√©quences

| Paire       | Fr√©quence |
| ----------- | --------- |
| (u, g)      | 3         |
| (h, u)      | 2         |
| (g, `</w>`) | 2         |
| (p, u)      | 2         |
| (u, n)      | 2         |
| (n, `</w>`) | 2         |
| (b, u)      | 1         |
| (g, s)      | 1         |
| (s, `</w>`) | 1         |

On observe que la paire la plus fr√©quente est (u, g) donc on la fusionne dans notre corpus.

```python
New_Corpus = ['h', 'ug', '</w>','p', 'ug', '</w>','p', 'u', 'n', '</w>','b', 'u', 'n', '</w>','h', 'ug', 's', '</w>']
```

Puis on recommence 

| Paire        | Fr√©quence |
| ------------ | --------- |
| (h, ug)      | 2         |
| (ug, `</w>`) | 2         |
| (u, n)       | 2         |
| (n, `</w>`)  | 2         |
| (p, ug)      | 1         |
| (s, `</w>`)  | 1         |
| (ug, s)      | 1         |
| (p, u)       | 1         |
| (b, u)       | 1         |

On voit que plusieurs paires ont la m√™me fr√©quence donc on d√©cide de prendre la premi√®re dans l‚Äôordre alphab√©tique c‚Äôest √† dire (h, ug), puis fusionner dans notre corpus.

```python
New_Corpus = ['hug', '</w>', 'p', 'ug', '</w>','p', 'u', 'n', '</w>','b', 'u', 'n', '</w>','hug', 's', '</w>']
```

Puis on r√©p√®te le processus jusqu‚Äô√† atteindre la taille de vocabulaire souhait√©e. Dans cet exemple, une taille de vocabulaire de 14 tokens.

```python
New_Corpus = ['hug', '</w>', 'p', 'ug', '</w>','p', 'un</w>','b','hug', 's', '</w>']

vocab = {0: "</w>", 1: "b", 2: "g", 3: "h", 4: "n", 5: "p", 6: "s", 7: "u", 8: 'ug', 9: 'hug', 10: 'un</w>', 11: "[CLS]", 12: "[SEP]", 13: "[UNK]", 14: "[PAD]"}
```

Maintenant imaginons que je veuille tokeniser le mots "hugs"

‚û°Ô∏è `["[CLS]", "hug", "s", "</w>", "[SEP]"]`
‚û°Ô∏è  <mark style="background: #CACFD9A6;">[11, 9, 6, 0, 12]</mark>

Le r√©sultat est donc `[11, 9, 6, 0, 12]`

La premi√®re fois que j‚Äôai entendu parler du terme BPE, c‚Äô√©tait dans une vid√©o YouTube qui parlait de la compression de donn√©es car √† la base BPE √©tait une technique de compression.

## Enfin la tokenisation Byte-Level BPE

La tokenisation Byte-Level BPE est une variante de la tokenisation Byte Pair Encoding. Sauf que contrairement au BPE, l'algorithme n‚Äôest pas appliqu√© sur du **texte brut** mais sur des octets. **Chaque caract√®re** (que ce soit les √©mojis ou des lettres accentu√©es) est **converti en byte**, une valeur num√©rique entre 0 et 255.  

C‚Äôest la tokenisation Byte-Level BPE qui est **utilis√©e dans les mod√®les GPT**. Cela signifie que GPT n‚Äôapprend pas √† partir de mots ou de lettres mais √† partir de s√©quences de bytes.

**Pourquoi ce choix ?**

Tous les textes peuvent √™tre repr√©sent√©s sous forme de bytes. Donc pas de OOV, c‚Äôest √† dire de *Out of Vocabulary*, **aucun mots qui ne sera pas pris en compte par le mod√®le**.

**Il n‚Äôexiste pas de symbole ou caract√®re que l‚Äôon ne puisse pas repr√©senter en bytes**. Comme notre vocabulaire contient au minimum les 256 bytes possibles, il n‚Äôy aura jamais de *Out of Vocabulary*.

En contrepartie, il y a **plus de tokens par phrase** et il est **plus difficile pour le mod√®le de comprendre le sens des mots** car les mots sont sous forme de s√©quences de bytes.

## Fonctionnement 

1. On commence avec un vocabulaire contenant les 256 bytes possibles + les tokens sp√©ciaux. 

GPT-2 utilise un seul vrai jeton sp√©cial "\`\`"  qui marque la fin d'un texte. Les espaces sont repr√©sent√©s par "ƒ†+mots" (ex: "ƒ†voiture") et les sauts de lignes par "ƒä".

2. Le corpus est encod√© en bytes (UTF-8).

3. Ces bytes sont regroup√©s par paires gr√¢ce √† l‚Äôalgorithme BPE (Byte Pair Encoding).

4. Le processus recommence jusqu‚Äô√† un nombre de token pr√©cis dans le vocabulaire

### Bonus : Impl√©mentation en python

On commence par installer tiktoken une librairie d√©velopp√©e et utilis√©e par open ai pour faire de la tokenisation de type Byte Level BPE.

```
!pip install tiktoken
```

Tiktoken est tr√®s simple √† utiliser tout en √©tant tr√®s performant.

```python
import tiktoken

#on utilise le tokenizer de gpt2
enc = tiktoken.get_encoding("gpt2")

#phrase que nous allons tokeniser
sentence = "Le chat est sur le canap√© üôÄ."

#on va tokeniser notre phrase
text_tokenize = enc.encode(sentence)
print(text_tokenize)
#[3123, 8537, 1556, 969, 443, 460, 499, 2634, 12520, 247, 222, 13]

#on peut la d√©tokeniser
decoded = enc.decode(text_tokenize)
print(decoded)
#Le chat est sur le canap√© üôÄ.

#on peut avoir un aper√ßu de la tokenisation
for t in text_tokenize:
¬† ¬† print(f"{t} ‚Üí '{enc.decode([t])}'", end=", ")
#3123 ‚Üí 'Le', 8537 ‚Üí ' chat', 1556 ‚Üí ' est', 969 ‚Üí ' sur', 443 ‚Üí ' le', 460 ‚Üí ' can', 499 ‚Üí 'ap', 2634 ‚Üí '√©', 12520 ‚Üí ' ÔøΩ', 247 ‚Üí 'ÔøΩ', 222 ‚Üí 'ÔøΩ', 13 ‚Üí '.',

#on observe que l‚Äôemoji üôÄ est repr√©sent√© par plusieurs bytes car il n‚Äôest pas enti√®rement dans le vocabulaire donc pour le tokeniser il est divis√© en 3 sequences de bytes.
```

# Embeddings : Repr√©sentation vectorielle des tokens

On passe maintenant √† une des parties les plus int√©ressantes le **word embedding**.

Le word embedding permet de **transformer des mots en vecteur pour capturer leur sens**. Cela permet par exemple de faire des op√©rations avec les mots comme: 

```python
roi - homme + femme ‚âà reine
 ```

ü´∑  **L'embedding ne s'applique pas uniquement aux mots mais peut aussi s'appliquer sur des tokens**

Mais comment capturer le sens d'un mot ?

L'embedding est bas√© sur l'**hypoth√®se distributionnelle**, une th√©orie √©nonc√©e pour la premi√®re fois par Zellig Harris dans [<u>Distributional Structure</u>](https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520) en 1954 qui dit que *"**les diff√©rences de sens sont corr√©l√©es aux diff√©rences de distribution**"*. En d'autres termes **les mots qui apparaissent dans le m√™me contexte ont tendance √† avoir des significations similaires** donc √† partir du moment o√π l'on a le contexte d'un mots on peut avoir son sens. Par exemple si le mot "**voiture**" est statistiquement **entour√© des m√™mes mots** que "**automobile**" ils ont un **sens proche**.

## La cooccurrence 

La **cooccurrence** d√©signe le fait que **deux mots apparaissent dans un m√™me contexte**. 

Dans notre exemple pr√©c√©dent on dit que "**voiture**" est statistiquement **entour√© des m√™me mots** que "**automobile**" donc "automobile" et "voiture" **sont en cooccurrence**. 

√Ä partir de l'**hypoth√®se distributionnelle** on peut d√©j√† faire de l'embedding simple. Prenons 2 phrases.

```python
"Le chat est sur le canap√©.", "Le chat mange la souris."
```

Maintenant nous allons pour chaque mot noter son contexte textuel avec une fen√™tre de taille 1 c'est √† dire que le contexte est de 1 mot autour du mot dont on cherche le contexte. Dans cet exemple imaginons que on cherche le contexte de mot "**est**". Nous voyons alors que les mots "**chat**" et "**sur**" font parti du contexte du mot "**est**". 

```python
Le "chat" *est* "sur" le canap√©.
```

Voici un tableau qui montre le contexte de chaque mot dans nos deux phrases, on peut dire que c'est une matrice de cooccurrence. Pour chaque mot de chaque colonne, √† chaque fois qu'un mot est observ√© dans son contexte on ajoute 1 √† la ligne correspondante. 

|        | le  | chat | est | mange | sur | la  | canap√© | souris |
| ------ | --- | ---- | --- | ----- | --- | --- | ------ | ------ |
| le     | 0   | 2    | 0   | 0     | 1   | 0   | 1      | 0      |
| chat   | 2   | 0    | 1   | 1     | 0   | 0   | 0      | 0      |
| est    | 0   | 1    | 0   | 0     | 1   | 0   | 0      | 0      |
| mange  | 0   | 1    | 0   | 0     | 0   | 1   | 0      | 0      |
| sur    | 1   | 0    | 1   | 0     | 0   | 0   | 0      | 0      |
| la     | 0   | 0    | 0   | 1     | 0   | 0   | 0      | 1      |
| canap√© | 1   | 0    | 0   | 0     | 0   | 0   | 0      | 0      |
| souris | 0   | 0    | 0   | 0     | 0   | 1   | 0      | 0      |
√Ä partir de cette matrice, chaque mot peut √™tre repr√©sent√© par un vecteur en 8 dimensions, correspondant aux 8 mots du vocabulaire.

Par exemple pour le mot **"est"** son vecteur est : `[0, 1, 0, 0, 1, 0, 0, 0]`.

Le **principal probl√®me** de cette approche est qu'elle **favorise les mots les plus fr√©quents**. **Les mots qui apparaissent le plus souvent** comme 'le' seront en **cooccurrence avec la plupart des mots.** Pourtant ils n'apportent pas beaucoup de sens √† la phrase. Cette approche **ne permet pas de mettre en valeur les mots avec le plus d'information s√©mantique.**

## Stop word

Les **mots** qui apportent **tr√®s peu d'informations s√©mantiques** sont appel√©s des **mots vides**. Pour **certains types d'embeddings**, ils peuvent **tout simplement √™tre supprim√©s**, mais ce n'est pas le cas pour les mod√®les de langage, car on veut qu'ils soient capables d'interpr√©ter le plus de mots possible.

### TF-IDF (Term Frequency - Inverse Document Frequency) 

Les matrices de cooccurrence brute **survalorisent** donc les **mots tr√®s fr√©quents** comme "le" ou "et" au **d√©triment des mots** qui apporteraient **plus d'information** s√©mantique √† la phrase.

Pour **palier ce probl√®me** appara√Æt le **concept de TF-IDF** (Term Frequency - Inverse Document Frequency). 

Pour faire simple avec le concept de TF-IDF **plus un mot est fr√©quent dans la phrase et plus il est rare dans l‚Äôensemble du corpus, plus il est jug√© important**.

Si on reprends nos deux phrases.

```python
"Le chat est sur le canap√©.", "Le chat mange la souris."
```

Si on applique TF-IDF on arrive √† ce r√©sultat

```python
canap√©   -> [0.42519636 0.        ]
chat     -> [0.30253071 0.35520009]
est      -> [0.42519636 0.        ]
la       -> [0.         0.49922133]
le       -> [0.60506143 0.35520009]
mange    -> [0.         0.49922133]
souris   -> [0.         0.49922133]
sur      -> [0.42519636 0.        ]

#Cet exemple est exag√©r√©, car l'algorithme TF-IDF doit √™tre appliqu√© sur un corpus plus grand que deux phrases.
```

L'approche de la cooccurrence associ√©e au concept de TF-IDF est une approche simple de l'embedding. Il y a une augmentation rapide du nombre de dimensions et les vecteurs sont creux, c'est-√†-dire que la plupart des informations que contiennent les vecteurs sont des 0.

Bonus : Comment calculer le TF-IDF 

Le **TF-IDF** est le produit de **TF √ó IDF**.

$\mathrm{TFIDF}(t, d, D) = \mathrm{TF}(t, d) \times \mathrm{IDF}(t, D)$

Dans cette expression:

- **$t$** repr√©sente un mots 
- **$d$** repr√©sente le document
- **$D$** repr√©sente l'ensemble du corpus

La **term frequency** est calcul√© gr√¢ce √† cette expression:

$\mathrm{TF}(t, d) = \frac{\text{Nombre d'occurrences du terme } t \text{ dans le document } d}{\text{Nombre total de mots dans le document } d}$

L'**inverse document frequency** est calcul√© gr√¢ce √† cette expression:

$\text{IDF}(t, D) = \log\left(\frac{N}{\text{df}(t)}\right)$

Dans cette expression:

- **$N$** le nombre de documents dans le corpus
- **$df(t)$** repr√©sente le nombre de documents dans lequel le mot $t$ fait son apparition

## Les embeddings appris   

Avec les **embeddings appris**, chaque **mot** se voit **attribuer** un vecteur **fixe** **lors de son apprentissage**.

Il existe **deux m√©thodes d'apprentissage populaires**. 

**CBOW (Continuous Bag of Words)**: CBOW cherche √† pr√©dire un mot cible √† partir de son contexte textuel.

**Skip-gram**: C'est l'inverse de CBOW, avec Skip-gram on cherche √† pr√©dire le contexte textuel d'un mot cible.

Un des mod√®les d'embedding appris les plus populaires est le mod√®le **Word2Vec** d√©velopp√© par Google.

### Les Inconv√©nients de cette approche

On observe **deux inconv√©nients** principaux √† cette approche.

1. Cette approche ne g√®re pas le **polys√©mantisme**, le mot souris sera le m√™me qu'il d√©signe l'animal ou le p√©riph√©rique de l'ordinateur. 

2. Avec cette approche la **taille du vocabulaire** est verrouill√©e, pas de *Out of Vocabulary*. 

Nous ne rentrerons **pas plus dans les d√©tails** car les embeddings appris ne sont pas les plus adapt√©s **pour les mod√®les de langage**. N√©anmoins pour en savoir plus sur le fonctionnement je vous conseille cet [article](https://datascientest.com/nlp-word-embedding-word2vec).

üößEn travaux üöß
