# Le fonctionnement des LLM de type GPT

# Introduction

Dans ce guide je vais essayer de vous faire comprendre de faÃ§on dÃ©taillÃ©e comment fonctionne le systÃ¨me GPT (**generative pre-trained transformer**) il en existe de plein de type qui ont des fonctions diffÃ©rentes (BioGPT (biomÃ©decine) [ProGPT2](https://huggingface.co/nferruz/ProtGPT2), [ChatGPT](https://chatgpt.com/) (modÃ¨le gÃ©nÃ©ral.) mais nous allons nous focaliser plutÃ´t sur une architecture proche de GPT 2.

Le seul prÃ©requis est de maitriser les bases de python et dâ€™avoir dÃ©jÃ  utilisÃ© [ChatGPT](https://chatgpt.com/). Lâ€™idÃ©e de ce guide est dâ€™Ãªtre simple et dÃ©taillÃ© Ã  la fois en vulgarisant le moins possible.

# Les Datasets

## Quâ€™est-ce quâ€™un dataset et Ã  quoi Ã§a sert ?

Un dataset est un **ensemble de donnÃ©es structurÃ©es**, comme par exemple :

- un ensemble de photos de pingouins ğŸ§,

- un ensemble de tous les Ã©crits de Shakespeare ğŸ“–,

- ou encore une liste de tous les repas dâ€™un individu pendant un an ğŸ¥˜.

Ces donnÃ©es peuvent Ãªtre **labellisÃ©es**, câ€™est-Ã -dire que chaque donnÃ©e est associÃ©e Ã  une Ã©tiquette. Si on reprend lâ€™exemple du dataset contenant tous les repas dâ€™un individu pendant un an, chaque repas peut Ãªtre labellisÃ© en associant le jour et le moment de la journÃ©e oÃ¹ le repas a Ã©tÃ© pris.

Les datasets sont utilisÃ©s dans divers domaines comme le **machine learning** ou encore la **crÃ©ation de bases de donnÃ©es**.

Quand un dataset contient un ensemble de textes on parle de **corpus**.
## Quel rÃ´le jouent les datasets dans lâ€™entraÃ®nement des modÃ¨les GPT ?


Les datasets sont le composant le plus important des modÃ¨les de langage car ils forgent **le style et la qualitÃ© du langage** et **les connaissances dâ€™un modÃ¨le**.
## WebText

Les modÃ¨les au-delÃ  de GPT 2 de chez open ai nâ€™ont pas Ã©tÃ© entraÃ®nÃ©s sur des datasets comme WikipÃ©dia mais sur **WebText**.

WebText contient le contenu de tous les **liens sortants de Reddit avec au moins 3 karmas (likes)**.

WebText est basÃ© sur **45 millions de liens**, dont **8 millions de documents** pour un total de **40 Go de texte**.

OpenAI a fait Ã§a pour mettre lâ€™**accent sur la qualitÃ© des donnÃ©es**.

Il est important de noter que tous les documents provenant de WikipÃ©dia ne font pas partie de WebText.

Cela est dÃ» Ã  deux raisons principales :

- Pour diversifier la langue (WikipÃ©dia a un langage neutre et trÃ¨s structurÃ©),

- Pour Ã©viter les rÃ©pÃ©titions de donnÃ©es.

WebText **nâ€™est pas open source** (public), mais il existe des datasets similaires open source comme [OpenWebText](https://github.com/jcpeterson/openwebtext) ou encore [The Pile](https://pile.eleuther.ai/).

## Liste de datasets que vous pouvez utiliser 

On choisit souvent un dataset en fonction de **sa taille** et du **type de langage** souhaitÃ©.

Datasets lÃ©gers: 

- **Tiny Shakespeare** : [https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) (lÃ©ger, il est conseillÃ© pour faire des petits modÃ¨les)

- **Wikipedia** : [https://huggingface.co/datasets/openskyml/wikipedia](https://huggingface.co/datasets/openskyml/wikipedia) (disponible en plusieurs langages)

- **Open Artificial Knowledge** : https://oakdataset.org/ (les donnÃ©es ont Ã©tÃ© rÃ©cupÃ©rÃ©es grÃ¢ce Ã  des IA comme ChatGPT, Claude et Gemini)

Datasets plus lourds: 

- **OpenWebText** : https://github.com/jcpeterson/openwebtext (version open source de WebText de 38 Go)

- **The Pile** : https://pile.eleuther.ai (corpus de 886 go utilisÃ© pour des modÃ¨les comme GPT-Neo et LLaMa)

# Tokenisation 

## La Tokenisation câ€™est quoi ?

Pour comprendre un texte, notre modÃ¨le GPT a besoin dâ€™**effectuer des opÃ©rations mathÃ©matiques sur les mots**. Mais sur du texte brut câ€™est juste impossible. Pour cela on a besoin de **tokeniser** notre texte. Câ€™est Ã  dire quâ€™on va **transformer et dÃ©couper en unitÃ©s le texte pour lâ€™adapter** Ã  notre modÃ¨le. On dit que le texte est dÃ©coupÃ© en **token**.

Dans le cas dâ€™un modÃ¨le comme GPT, celui-ci va dans un premier temps **prendre un corpus de texte et le diviser en unitÃ©s**. 
Chaque unitÃ© est ajoutÃ©e Ã  une **liste de vocabulaire** **avec un identifiant** sâ€™il nâ€™existe pas. 

Puis notre modÃ¨le est **prÃªt** Ã  tokeniser. Le texte sera dÃ©coupÃ© en unitÃ©s chaque unitÃ© dans le texte est remplacÃ©e par le numÃ©ro associÃ©. 

![Miro Image](https://miro.medium.com/v2/resize:fit:828/format:webp/1*gWP5Whykah1101EpYy17qQ.png)

*Source Image : https://teetracker.medium.com/llm-fine-tuning-step-tokenizing-caebb280cfc2

La question principale est donc comment allons-nous dÃ©couper notre texte. Nous allons explorer les diffÃ©rentes approches qui sâ€™offrent Ã  nous.
## Les tokens spÃ©ciaux dans la tokenisation

Dans le vocabulaire en plus des unitÃ©s il y a des tokens spÃ©ciaux en voici une liste:

- `[CLS]` / `[SEP]` Marquent le **dÃ©but/fin** de la sÃ©quence
- `[UNK]` Remplace **les unitÃ©s inconnues** dans le vocabulaire
- `[PAD]` Lors de lâ€™entraÃ®nement dâ€™un modÃ¨le si l'on veut pouvoir gÃ©rer **plusieurs phrases** en mÃªme temps il faut quâ€™elles aient la **mÃªme longueur**. Pour cela, on ajoute le token `[PAD]` Ã  la fin des phrases les plus courtes. Il faudra quand mÃªme spÃ©cifier au modÃ¨le de ne pas faire attention Ã  `[PAD]` et lâ€™ignorer dans son entraÃ®nement. 

Exemple `[PAD]`:
```
sÃ©quence 1 : [[CLS], "Bonjour", "tout", "le", "monde", [SEP]]  
sÃ©quence 2 : [[CLS], "Salut", [SEP], [PAD], [PAD], [PAD]]
```

### Bonus technique:

Pourquoi faut il que les phrases aient la mÃªme longueur quand elles sont par lot ?

1. Les tenseurs **ne peuvent pas avoir des lignes de longueurs diffÃ©rentes**

Les tenseurs sont des **types dâ€™objets optimisÃ©s pour les calculs en python** et ils ne peuvent pas contenir des lignes avec des tailles diffÃ©rentes.

2. On doit **connaÃ®tre Ã  lâ€™avance les dimensions dâ€™une matrice** si on veut faire un produit matriciel ou un softmax

Nous verrons plus tard que pour notre modÃ¨le Transformer le modÃ¨le doit connaÃ®tre Ã  lâ€™avance la **taille du batch** (la taille du lot que notre modÃ¨le va â€™absorberâ€™ Ã  chaque Ã©tape de notre entraÃ®nement), justement la **taille des sÃ©quences dans les lots** et enfin les **dimensions vectorielles de chaque token** mais nous verrons cela plus en dÃ©tail plus tard.

### La Tokenisation par mot

Une mÃ©thode est de **diviser notre texte par mots**.

On commence par dÃ©finir notre vocabulaire pour cela, imaginons que nous prenons juste une phrase

```python
"Le chat mange la souris"
```

Le vocabulaire sera donc : 

```python
{0: 'souris', 1: 'chat', 2: 'le', 3: 'la', 4: '.', 5: 'mange', 6: '[UNK]', 7: '[PAD]', 8: '[CLS]', 9: '[SEP]'}
```

Imaginons que nous essayons de tokeniser avec notre modÃ¨le la phrase <mark style="background: #BBFABBA6;">"La souris mange le chat."</mark>, le rÃ©sultat sera donc <mark style="background: #BBFABBA6;">[8, 2, 1, 6, 6, 2, 6, 4, 9]</mark>

```python
â¡ï¸ "La souris mange le chat."
â¡ï¸ ['[CLS]', 'la', 'souris', 'mange', 'le', 'chat', '.', '[SEP]']
â¡ï¸ [8, 3, 0, 5, 2, 1, 4, 9]
```

### Les problÃ¨mes de cette mÃ©thode

Prenons une autre phrase :

<mark style="background: #FFB86CA6;">"Le chat est sur le canapÃ©."</mark>

Le rÃ©sultat de la tokenisation sera :  
<mark style="background: #FF5582A6;">['[CLS]', 'le', 'chat', '[UNK]', '[UNK]', 'le', '[UNK]', '.', '[SEP]']</mark>  

ce qui donne :  
<mark style="background: #CACFD9A6">[8, 2, 1, 6, 6, 2, 6, 4, 9]</mark>

Si un mot nâ€™est **pas exactement dans le vocabulaire**, il sera totalement **perdu et remplacÃ©** par `[UNK]`ce qui constitue une **perte dâ€™information** pour notre modÃ¨le. Le **vocabulaire a donc besoin de contenir Ã©normÃ©ment de mots**, ce qui le rendrait Ã©norme, ce qui est un inconvÃ©nient pour les calculs car il occuperait **beaucoup de mÃ©moire**. De plus le modÃ¨le **ne comprendrait pas les mots avec des fautes dâ€™orthographe**. 

### Bonus : ImplÃ©mentation en python

```python
import re

#Prenons une seule phrase comme corpus
corpus = "Le chat mange la souris."
  
#On va sÃ©parer les ponctuations
corpus = re.sub(r"([?!.,;:])", r" \1 ", corpus)

#Imaginons quâ€™une unitÃ© corresponde Ã  un mot
#CommenÃ§ons par dÃ©finir un vocabulaire associant chaque mot Ã  un numÃ©ro
vocabulaire = {i: word for i, word in enumerate(set(corpus.lower().split()))}
#âš ï¸ Jâ€™utilise set donc lâ€™ordre des tokens normaux nâ€™est pas dÃ©terministe (car un set en Python est implÃ©mentÃ© comme une table de hachage)

#Puis ajoutons les tokens spÃ©ciaux
vocabulaire[len(vocabulaire)] = '[UNK]' #pour les mots inconnus
vocabulaire[len(vocabulaire)] = '[PAD]' #pour le padding
vocabulaire[len(vocabulaire)] = '[CLS]' #pour le dÃ©but de phrase
vocabulaire[len(vocabulaire)] = '[SEP]' #pour la fin de phrase

#Notre vocabulaire est donc {0: 'souris', 1: 'chat', 2: 'le', 3: 'la', 4: '.', 5: 'mange', 6: '[UNK]', 7: '[PAD]', 8: '[CLS]', 9: '[SEP]'}

vocabulaire_inverse = {word: i for i, word in vocabulaire.items()}
#{'souris': 0, 'chat': 1, 'le': 2, 'la': 3, '.': 4, 'mange': 5, '[UNK]': 6, '[PAD]': 7, '[CLS]': 8, '[SEP]': 9}

#puis dÃ©finissons une fonction qui va dÃ©couper et transformer notre texte
def tokenize(text):
Â  Â  #On va dâ€™abord le mettre en minuscule
Â  Â  text = text.lower()

Â  Â  #On va ensuite sÃ©parer les ponctuations
Â  Â  text = re.sub(r"([?!.,;:])", r" \1 ", text)

Â  Â  #On va ensuite sÃ©parer les mots
Â  Â  text = text.split()

Â  Â  #on va ajouter les tokens spÃ©ciaux
Â  Â  text = ['[CLS]'] + ['[UNK]' if w not in vocabulaire_inverse else w for w in text] + ['[SEP]']

Â  Â  #On va ensuite transformer les mots en id
Â  Â  text_tokenise = [vocabulaire_inverse[word] for word in text]

Â  Â  return text_tokenise
  
print(tokenize("La souris mange le chat."))
#['[CLS]', 'la', 'souris', 'mange', 'le', 'chat', '.', '[SEP]'] Â => [8, 3, 0, 5, 2, 1, 4, 9]

print(tokenize("Le chat est sur le canapÃ©."))
#['[CLS]', 'le', 'chat', '[UNK]', '[UNK]', 'le', '[UNK]', '.', '[SEP]'] => [8, 2, 1, 6, 6, 2, 6, 4, 9]
```

## La Tokenisation par caractÃ¨re

Une autre approche est la tokenisation par caractÃ¨re. Câ€™est Ã  dire de **diviser le texte par caractÃ¨re Ã  la place de mots**. Cette approche **rÃ©duirait considÃ©rablement la taille du vocabulaire et garantit que la plupart du vocabulaire soit reconnu**.

### Les InconvÃ©nients de cette approche 

Il y a deux gros inconvÃ©nients:

- Les sÃ©quences deviennent trÃ¨s longues car elles sont dÃ©coupÃ©s plus finement.  
â¡ï¸ <mark style="background: #FFB86CA6;">"Le chat est sur le canapÃ©."</mark>
â¡ï¸ <mark style="background: #CACFD9A6;">[36, 17, 10, 0, 8, 13, 6, 25, 0, 10, 24, 25, 0, 24, 26, 23, 0, 17, 10, 0, 8, 6, 19, 6, 21, 32, 0, 4, 0, 37]</mark>

- Câ€™est plus compliquÃ© pour un modÃ¨le de comprendre la relation entre les caractÃ¨res que la relation entre les mots.
### Bonus : ImplÃ©mentation en python

```python
import re

#Prenons comme corpus seulement une phrase
corpus = "Hier, au zoo, jâ€™ai vu dix guÃ©pards, cinq zÃ©bus, un yak et le wapiti fumer. ?!â€¦"
#Cette phrase a la particularitÃ© de contenir toutes les lettres de l'alphabet et quelques ponctuations

#On va sÃ©parer les ponctuations
corpus = re.sub(r"([?!.,;:])", r" \1 ", corpus)

#On commence par dÃ©finir un vocabulaire en associant un identifiant Ã  chaque caractÃ¨re
vocabulaire = {i : word for i, word in enumerate(sorted(set(corpus.lower())))}
#{0: ' ', 1: '!', 2: "'", 3: ',', 4: '.', 5: '?', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z', 32: 'Ã©', 33: 'â€¦'}

#Puis ajoutons les tokens spÃ©ciaux
vocabulaire[len(vocabulaire)] = '[UNK]' #pour les mots inconnus
vocabulaire[len(vocabulaire)] = '[PAD]' #pour le padding
vocabulaire[len(vocabulaire)] = '[CLS]' #pour le dÃ©but de phrase
vocabulaire[len(vocabulaire)] = '[SEP]' #pour la fin de phrase

#Nous nous retrouvons donc avec un vocabulaire de 37 Ã©lÃ©ments
#{0: ' ', 1: '!', 2: "'", 3: ',', 4: '.', 5: '?', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z', 32: 'Ã©', 33: 'â€¦', 34: '[UNK]', 35: '[PAD]', 36: '[CLS]', 37: '[SEP]'}

vocabulaire_inverse = {word: i for i, word in vocabulaire.items()}

#puis dÃ©finissons une fonction qui va dÃ©couper et transformer notre texte
def tokenize(text):
Â  Â  #On va dâ€™abord le mettre en minuscule
Â  Â  text = text.lower()

Â  Â  #On va ensuite sÃ©parer les ponctuations
Â  Â  text = re.sub(r"([?!.,;:])", r" \1 ", text)

Â  Â  #on va ajouter les tokens spÃ©ciaux
Â  Â  text = ['[CLS]'] + ['[UNK]' if w not in vocabulaire_inverse else w for w in text] + ['[SEP]']

Â  Â  #On va ensuite transformer les mots en id
Â  Â  text_tokenise = [vocabulaire_inverse[word] for word in text]

Â  Â  return text_tokenise

print(tokenize("Le chat mange la souris."))
# => ['[CLS]', 'l', 'e', ' ', 'c', 'h', 'a', 't', ' ', 'm', 'a', 'n', 'g', 'e', ' ', 'l', 'a', ' ', 's', 'o', 'u', 'r', 'i', 's', ' ', '.', ' ', '[SEP]']
# => [36, 17, 10, 0, 8, 13, 6, 25, 0, 18, 6, 19, 12, 10, 0, 17, 6, 0, 24, 20, 26, 23, 14, 24, 0, 4, 0, 37]

print(tokenize("Le chat est sur le canapÃ©."))
# => ['[CLS]', 'l', 'e', ' ', 'c', 'h', 'a', 't', ' ', 'e', 's', 't', ' ', 's', 'u', 'r', ' ', 'l', 'e', ' ', 'c', 'a', 'n', 'a', 'p', 'Ã©', ' ', '.', ' ', '[SEP]']
# => [36, 17, 10, 0, 8, 13, 6, 25, 0, 10, 24, 25, 0, 24, 26, 23, 0, 17, 10, 0, 8, 6, 19, 6, 21, 32, 0, 4, 0, 37]
```

## Tokenisation BPE (Byte Pair Encoding) 

Le BPE est une mÃ©thode de tokenisation par **subwords** (câ€™est-Ã -dire **des sous-mots**). Au dÃ©but, **BPE dÃ©coupe en caractÃ¨re et petit Ã  petit ajoute les paires les plus frÃ©quentes**. Ainsi, BPE a pour objectif de rÃ©duire le nombre de tokens nÃ©cessaire pour tokeniser une phrase sans perdre en prÃ©cision.

On commence avec un vocabulaire avec tous les caractÃ¨res de base du corpus et on traite le corpus pour quâ€™il soit reprÃ©sentÃ© comme une liste de tokens

```python
Corpus = "hug", "pug", "pun", "bun", "hugs"

New_Corpus = ['h', 'uâ€™, â€™gâ€™, â€™</w>â€™,â€™pâ€™, â€™uâ€™, â€™gâ€™, â€™</w>â€™,â€™pâ€™, â€™uâ€™, â€™nâ€™, â€™</w>â€™,â€™bâ€™, â€™uâ€™, â€™nâ€™, â€™</w>â€™,â€™hâ€™, â€™uâ€™, â€™gâ€™, â€™sâ€™, â€™</w>â€™]

notre_vocabulaire_de_base = ["b", "g", "h", "n", "p", "s", "u"]
```

Puis on se met Ã  compter les paires et leurs frÃ©quences

| Paire       | FrÃ©quence |
| ----------- | --------- |
| (u, g)      | 3         |
| (h, u)      | 2         |
| (g, `</w>`) | 2         |
| (p, u)      | 2         |
| (u, n)      | 2         |
| (n, `</w>`) | 2         |
| (b, u)      | 1         |
| (g, s)      | 1         |
| (s, `</w>`) | 1         |

On observe que la paire la plus frÃ©quente est (u, g) donc on la fusionne dans notre corpus.

```python
New_Corpus = ['h', 'ug', '</w>','p', 'ug', '</w>','p', 'u', 'n', '</w>','b', 'u', 'n', '</w>','h', 'ug', 's', '</w>']
```

Puis on recommence 

| Paire        | FrÃ©quence |
| ------------ | --------- |
| (h, ug)      | 2         |
| (ug, `</w>`) | 2         |
| (u, n)       | 2         |
| (n, `</w>`)  | 2         |
| (p, ug)      | 1         |
| (s, `</w>`)  | 1         |
| (ug, s)      | 1         |
| (p, u)       | 1         |
| (b, u)       | 1         |

On voit que plusieurs paires ont la mÃªme frÃ©quence donc on dÃ©cide de prendre la premiÃ¨re dans lâ€™ordre alphabÃ©tique câ€™est Ã  dire (h, ug), puis fusionner dans notre corpus.

```python
New_Corpus = ['hug', '</w>', 'p', 'ug', '</w>','p', 'u', 'n', '</w>','b', 'u', 'n', '</w>','hug', 's', '</w>']
```

Puis on rÃ©pÃ¨te le processus jusquâ€™Ã  atteindre la taille de vocabulaire souhaitÃ©e. Dans cet exemple, une taille de vocabulaire de 14 tokens.

```python
New_Corpus = ['hug', '</w>', 'p', 'ug', '</w>','p', 'un</w>','b','hug', 's', '</w>']

vocab = {0: "</w>", 1: "b", 2: "g", 3: "h", 4: "n", 5: "p", 6: "s", 7: "u", 8: 'ug', 9: 'hug', 10: 'un</w>', 11: "[CLS]", 12: "[SEP]", 13: "[UNK]", 14: "[PAD]"}
```

Maintenant imaginons que je veuille tokeniser le mots "hugs"

â¡ï¸ `["[CLS]", "hug", "s", "</w>", "[SEP]"]`
â¡ï¸  <mark style="background: #CACFD9A6;">[11, 9, 6, 0, 12]</mark>

Le rÃ©sultat est donc `[11, 9, 6, 0, 12]`

La premiÃ¨re fois que jâ€™ai entendu parler du terme BPE, câ€™Ã©tait dans une vidÃ©o YouTube qui parlait de la compression de donnÃ©es car Ã  la base BPE Ã©tait une technique de compression.

## Enfin la tokenisation Byte-Level BPE

La tokenisation Byte-Level BPE est une variante de la tokenisation Byte Pair Encoding. Sauf que contrairement au BPE, l'algorithme nâ€™est pas appliquÃ© sur du **texte brut** mais sur des octets. **Chaque caractÃ¨re** (que ce soit les Ã©mojis ou des lettres accentuÃ©es) est **converti en byte**, une valeur numÃ©rique entre 0 et 255.  

Câ€™est la tokenisation Byte-Level BPE qui est **utilisÃ©e dans les modÃ¨les GPT**. Cela signifie que GPT nâ€™apprend pas Ã  partir de mots ou de lettres mais Ã  partir de sÃ©quences de bytes.

**Pourquoi ce choix ?**

Tous les textes peuvent Ãªtre reprÃ©sentÃ©s sous forme de bytes. Donc pas de OOV, câ€™est Ã  dire de *Out of Vocabulary*, **aucun mots qui ne sera pas pris en compte par le modÃ¨le**.

**Il nâ€™existe pas de symbole ou caractÃ¨re que lâ€™on ne puisse pas reprÃ©senter en bytes**. Comme notre vocabulaire contient au minimum les 256 bytes possibles, il nâ€™y aura jamais de *Out of Vocabulary*.

En contrepartie, il y a **plus de tokens par phrase** et il est **plus difficile pour le modÃ¨le de comprendre le sens des mots** car les mots sont sous forme de sÃ©quences de bytes.

## Fonctionnement 

1. On commence avec un vocabulaire contenant les 256 bytes possibles + les tokens spÃ©ciaux. 

GPT-2 utilise un seul vrai jeton spÃ©cial "\`\`"  qui marque la fin d'un texte. Les espaces sont reprÃ©sentÃ©s par "Ä +mots" (ex: "Ä voiture") et les sauts de lignes par "ÄŠ".

2. Le corpus est encodÃ© en bytes (UTF-8).

3. Ces bytes sont regroupÃ©s par paires grÃ¢ce Ã  lâ€™algorithme BPE (Byte Pair Encoding).

4. Le processus recommence jusquâ€™Ã  un nombre de token prÃ©cis dans le vocabulaire

### Bonus : ImplÃ©mentation en python

On commence par installer tiktoken une librairie dÃ©veloppÃ©e et utilisÃ©e par open ai pour faire de la tokenisation de type Byte Level BPE.

```
!pip install tiktoken
```

Tiktoken est trÃ¨s simple Ã  utiliser tout en Ã©tant trÃ¨s performant.

```python
import tiktoken

#on utilise le tokenizer de gpt2
enc = tiktoken.get_encoding("gpt2")

#phrase que nous allons tokeniser
sentence = "Le chat est sur le canapÃ© ğŸ™€."

#on va tokeniser notre phrase
text_tokenize = enc.encode(sentence)
print(text_tokenize)
#[3123, 8537, 1556, 969, 443, 460, 499, 2634, 12520, 247, 222, 13]

#on peut la dÃ©tokeniser
decoded = enc.decode(text_tokenize)
print(decoded)
#Le chat est sur le canapÃ© ğŸ™€.

#on peut avoir un aperÃ§u de la tokenisation
for t in text_tokenize:
Â  Â  print(f"{t} â†’ '{enc.decode([t])}'", end=", ")
#3123 â†’ 'Le', 8537 â†’ ' chat', 1556 â†’ ' est', 969 â†’ ' sur', 443 â†’ ' le', 460 â†’ ' can', 499 â†’ 'ap', 2634 â†’ 'Ã©', 12520 â†’ ' ï¿½', 247 â†’ 'ï¿½', 222 â†’ 'ï¿½', 13 â†’ '.',

#on observe que lâ€™emoji ğŸ™€ est reprÃ©sentÃ© par plusieurs bytes car il nâ€™est pas entiÃ¨rement dans le vocabulaire donc pour le tokeniser il est divisÃ© en 3 sequences de bytes.
```

# Embeddings : ReprÃ©sentation vectorielle des tokens

On passe maintenant Ã  une des parties les plus intÃ©ressantes le **word embedding**.

Le word embedding permet de **transformer des mots en vecteur pour capturer leur sens**. Cela permet par exemple de faire des opÃ©rations avec les mots comme: 

```python
roi - homme + femme â‰ˆ reine
 ```

ğŸ«·  **L'embedding ne s'applique pas uniquement aux mots mais peut aussi s'appliquer sur des tokens**

Mais comment capturer le sens d'un mot ?

L'embedding est basÃ© sur l'**hypothÃ¨se distributionnelle**, une thÃ©orie Ã©noncÃ©e pour la premiÃ¨re fois par Zellig Harris dans [<u>Distributional Structure</u>](https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520) en 1954 qui dit que *"**les diffÃ©rences de sens sont corrÃ©lÃ©es aux diffÃ©rences de distribution**"*. En d'autres termes **les mots qui apparaissent dans le mÃªme contexte ont tendance Ã  avoir des significations similaires** donc Ã  partir du moment oÃ¹ l'on a le contexte d'un mots on peut avoir son sens. Par exemple si le mot "**voiture**" est statistiquement **entourÃ© des mÃªmes mots** que "**automobile**" ils ont un **sens proche**.

## La cooccurrence 

La **cooccurrence** dÃ©signe le fait que **deux mots apparaissent dans un mÃªme contexte**. 

Dans notre exemple prÃ©cÃ©dent on dit que "**voiture**" est statistiquement **entourÃ© des mÃªme mots** que "**automobile**" donc "automobile" et "voiture" **sont en cooccurrence**. 

Ã€ partir de l'**hypothÃ¨se distributionnelle** on peut dÃ©jÃ  faire de l'embedding simple. Prenons 2 phrases.

```python
"Le chat est sur le canapÃ©.", "Le chat mange la souris."
```

Maintenant nous allons pour chaque mot noter son contexte textuel avec une fenÃªtre de taille 1 c'est Ã  dire que le contexte est de 1 mot autour du mot dont on cherche le contexte. Dans cet exemple imaginons que on cherche le contexte de mot "**est**". Nous voyons alors que les mots "**chat**" et "**sur**" font parti du contexte du mot "**est**". 

```python
Le "chat" *est* "sur" le canapÃ©.
```

Voici un tableau qui montre le contexte de chaque mot dans nos deux phrases, on peut dire que c'est une matrice de cooccurrence. Pour chaque mot de chaque colonne, Ã  chaque fois qu'un mot est observÃ© dans son contexte on ajoute 1 Ã  la ligne correspondante. 

|        | le  | chat | est | mange | sur | la  | canapÃ© | souris |
| ------ | --- | ---- | --- | ----- | --- | --- | ------ | ------ |
| le     | 0   | 2    | 0   | 0     | 1   | 0   | 1      | 0      |
| chat   | 2   | 0    | 1   | 1     | 0   | 0   | 0      | 0      |
| est    | 0   | 1    | 0   | 0     | 1   | 0   | 0      | 0      |
| mange  | 0   | 1    | 0   | 0     | 0   | 1   | 0      | 0      |
| sur    | 1   | 0    | 1   | 0     | 0   | 0   | 0      | 0      |
| la     | 0   | 0    | 0   | 1     | 0   | 0   | 0      | 1      |
| canapÃ© | 1   | 0    | 0   | 0     | 0   | 0   | 0      | 0      |
| souris | 0   | 0    | 0   | 0     | 0   | 1   | 0      | 0      |
Ã€ partir de cette matrice, chaque mot peut Ãªtre reprÃ©sentÃ© par un vecteur en 8 dimensions, correspondant aux 8 mots du vocabulaire.

Par exemple pour le mot **"est"** son vecteur est : `[0, 1, 0, 0, 1, 0, 0, 0]`.

Le **principal problÃ¨me** de cette approche est qu'elle **favorise les mots les plus frÃ©quents**. **Les mots qui apparaissent le plus souvent** comme 'le' seront en **cooccurrence avec la plupart des mots.** Pourtant ils n'apportent pas beaucoup de sens Ã  la phrase. Cette approche **ne permet pas de mettre en valeur les mots avec le plus d'information sÃ©mantique.**

## Stop word

Les **mots** qui apportent **trÃ¨s peu d'informations sÃ©mantiques** sont appelÃ©s des **mots vides**. Pour **certains types d'embeddings**, ils peuvent **tout simplement Ãªtre supprimÃ©s**, mais ce n'est pas le cas pour les modÃ¨les de langage, car on veut qu'ils soient capables d'interprÃ©ter le plus de mots possible.

### TF-IDF (Term Frequency - Inverse Document Frequency) 

Les matrices de cooccurrence brute **survalorisent** donc les **mots trÃ¨s frÃ©quents** comme "le" ou "et" au **dÃ©triment des mots** qui apporteraient **plus d'information** sÃ©mantique Ã  la phrase.

Pour **palier ce problÃ¨me** apparaÃ®t le **concept de TF-IDF** (Term Frequency - Inverse Document Frequency). 

Pour faire simple avec le concept de TF-IDF **plus un mot est frÃ©quent dans la phrase et plus il est rare dans lâ€™ensemble du corpus, plus il est jugÃ© important**.

Si on reprends nos deux phrases.

```python
"Le chat est sur le canapÃ©.", "Le chat mange la souris."
```

Si on applique TF-IDF on arrive Ã  ce rÃ©sultat

```python
canapÃ©   -> [0.42519636 0.        ]
chat     -> [0.30253071 0.35520009]
est      -> [0.42519636 0.        ]
la       -> [0.         0.49922133]
le       -> [0.60506143 0.35520009]
mange    -> [0.         0.49922133]
souris   -> [0.         0.49922133]
sur      -> [0.42519636 0.        ]

#Cet exemple est exagÃ©rÃ©, car l'algorithme TF-IDF doit Ãªtre appliquÃ© sur un corpus plus grand que deux phrases.
```

L'approche de la cooccurrence associÃ©e au concept de TF-IDF est une approche simple de l'embedding. Il y a une augmentation rapide du nombre de dimensions et les vecteurs sont creux, c'est-Ã -dire que la plupart des informations que contiennent les vecteurs sont des 0.

### Bonus : Comment calculer le TF-IDF 

Le **TF-IDF** est le produit de **TF Ã— IDF**.

$$
\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
$$
Dans cette expression:

- **t** reprÃ©sente un mots 
- **d** reprÃ©sente le document
- **D** reprÃ©sente l'ensemble du corpus

La **term frequency** est calculÃ© grÃ¢ce Ã  cette expression:
$$
\text{TF}(t, d) = \frac{\text{nombre d'occurrences du terme } t \text{ dans le document } d}{\text{nombre total de mots dans le document } d}
$$

L'**inverse document frequency** est calculÃ© grÃ¢ce Ã  cette expression:
$$
\text{IDF}(t, D) = \log\left(\frac{N}{\text{$df$}(t)}\right)
$$
Dans cette expression:

- **N** le nombre de documents dans le corpus
- **$df(t)$** reprÃ©sente le nombre de documents dans lequels le mot t fait son apparition

ğŸš§En travaux ğŸš§
